\chapter{Data Management \& Data Access}

\section{HDFS}

\section{YARN}

\section{SQL: Hive}

Apache Hive is a relational database for Big Data, developed as a part of the Hadoop environment to provide fast access to huge data sets through its own query language, \textbf{HiveQL}, and through several possible execution engines such as Apache Tez, MapReduce and Apache Spark. Hive, as of recently, added  support for ACID transactions, making it viable as a data storage for more complex endeavours, including streaming applications.

\subsection{Components}

Hive architecture can be divided in five main components:

\begin{itemize}
    \item \textbf{Shell/UI}: Beeline is the frontend tool for interactive querying on the database. Hive supports connections via its own JDBC driver, allowing easy integration with clients and user applications.
    \item \textbf{Driver}: The component which receives the queries. This component implements the notion of session handles and provides execute and fetch APIs modeled on JDBC/ODBC interfaces.
    \item \textbf{Metastore}: It stores metadata about HDFS file locations and the table schemas, but also column and column type information and the serializers and deserializers necessary to read and write data.
    \item \textbf{Compiler}: It manages query parsing, planning and optimization, does semantic analysis on the different query blocks and query expressions generating, eventually, an execution plan with the help of the table and partition metadata looked up from the metastore. For what concerns the parsing, HiveQL is a SQL extension, compliant with the SQL:2011 standard.
    \item \textbf{Execution Engine}: Hive uses Apache Tez as a default execution engine  which allows low latency querying through its LLAP (Low Latency Analytical Processing) daemons: persistent processes running on YARN enabling the system to avoid the overhead caused by the container deployment for query executions. Other execution engines usable by Hive are, as already mentioned, MapReduce and Spark. The first one is needed, as of Hive 2.1, in order to use Hive Streaming API, while the other uses its own LLAP daemons, similarly to Tez, and still lags behind, performance wise.
\end{itemize}

\subsection{Hive Low Latency Processing on Tez}

Starting from Hive2, OLAP, OnLine Analytical Processing support has been introduced as default mode for query execution on top of Apache Tez.\\  
This functionality has been implemented on top of YARN where, while deploying the endpoint for the clients queries (HiveServer2), two applications are deployed: 

\begin{itemize}
    \item a Tez query coordinator application, which deals with a single query execution planning, a series of Map and Reduce operations, but also the concurrency of many queries, if needed;
    \item a Slider \footnote{\href{https://slider.incubator.apache.org/}{Apache Slider} is an application which allows dynamic deployment and monitoring of YARN applications} application that, together with the persistent daemons containers, deals with the actual processing of the single operation that needs to be executed.
\end{itemize}

With respect to the usual query execution, this architecture allows:

\begin{itemize} 
	\item a critical decrease in the latency caused by the creation of the YARN containers needed for the query execution, which is usually the biggest time consuming task.
	\item parallel and concurrent query execution, shared between all the daemons instances, taking also advantage of the In-Memory Cache of the single daemon, especially useful if different queries need to access the same data.
\end{itemize}

The introduction of LLAP/OLAP daemons provided Hive2 with an average 2600\% performance gain when compared to Hive1 using Tez, on a dataset of 1 TB.

\subsection{HiveQL}

HiveQL, which stands for Hive Query Language, is a SQL:2011 compliant language used for query expressions in Hive. Together with all the features coming from the SQL standard, it introduces concepts like external tables and table bucketing in its DDL\footnote{DDL: Data Definition Language, subset of a grammar which specifies the syntactic rules for the creation and alteration of objects such as databases, tables and indices.}, together with the possibility to use User Defined Functions for custom aggregations and operators.

\subsubsection{Data Definition Language}

HiveQL Data Definition Language allows creation and alteration of databases, tables and indices.\\When it comes to DBs, the classic statements \verb|CREATE|, \verb|ALTER| \& \verb|DROP| are available for creation, alteration and deletion of databases, together with the possibility to specify ownership, filesystem location and other custom properties.\\
Concerning tables, Hive DDL provides the ability to define external tables, located in the filesystem somewhere other than on the default warehouse, which can be read and queried like a normal table. In addition, it is possible to define constraints on column keys, such as foreign and primary keys, partitioning, bucketing, skewing and sorting for optimization purposes. It allows to define the kind of deserialization Hive needs to use in order to read the data stored, according to their file format with the \verb|ROW FORMAT| clause.

For example, the statement

\begin{minted}{SQL}
    CREATE TABLE IF NOT EXISTS 
    log_table(id string, count int, time timestamp)
    PARTITIONED BY (date string)
    CLUSTERED BY id SORTED BY (count DESC) INTO 10 BUCKETS
    SKEWED BY id ON 3, 4, 10
    STORED AS ORC;
\end{minted}

creates a table named "log\_table", using the default database, stored in ORC format partitioned according to a certain user-input string "date", in 10 ORC files sorted in descending order, where values are skewed on the id values 3, 4 and 10.

\subsubsection{Data Manipulation Language}

HiveQL Data Manipulation Language allows to modify data in Hive through multiple ways:

\begin{itemize}
    \item \verb|LOAD| allows file loading into Hive tables from HDFS or local filesystem.  
    \item \verb|INSERT| allows to insert query results into Hive tables or filesystem directories. In case it's needed, it is possible to overwrite or insert them into a dynamically created partition. 
    \item \verb|UPDATE| and \verb|DELETE| allow to update or delete values in tables supporting Hive Transactions.
    \item \verb|MERGE| allows to merge files belonging to the same table, if it supports Hive Transactions.
    \item \verb|IMPORT| and \verb|EXPORT| allow importing and exporting of both table values and metadata, to use them with other DBMS.
\end{itemize}

\subsection{Warehouse}
HDFS is the default physical storage of database and tables in Hive, but support for S3 and any other HDFS compatible filesystems is available. Since all of the files are stored on a distributed filesystem, redundancy and fault tolerance are granted when it comes to data integrity. Hive default storage format is ORC, a compressed format able to decrease file size up to 78\% with respect to normal Text Files.\\
Hive has built-in direct serialization and deserialization of CSV, JSON, AVRO and Parquet files as row formats, and allows to easily add custom SerDe\footnote{SerDe: Serialization/Deserialization} components.

\subsubsection{Hive Data Model}

Data in Hive is organized into 3 main abstractions: Tables, Partitions and Buckets.

\paragraph{Tables} These are analogous to Tables in Relational Databases. Tables can be filtered, projected, joined and unioned. Additionally all the data of a table is stored in a directory in its default warehouse, usually HDFS. Hive also supports the notion of external tables wherein a table can be created on pre-existing files or directories in HDFS by providing the appropriate location to the table creation DDL. The rows in a table are organized into typed columns similar to Relational Databases.
\paragraph{Partitions} Each Table can have one or more partition keys which determine how the data is stored, for example a table \verb|T| with a date partition column  \verb|ds| had files with data for a particular date stored in the  \verb|<table location>/ds=<date>| directory in HDFS. Partitions allow the system to prune data to be inspected based on query predicates, for example a query that is interested in rows from \verb|T| that satisfy the predicate \verb|T.ds='2008-09-01'| would only have to look at files in \verb|<table location>/ds=2008-09-01/| directory in HDFS.
\paragraph{Buckets} Data in each partition may, in turn, be divided into Buckets based on the hash of a column in the table. Each bucket is stored as a file in the partition directory. Bucketing allows the system to efficiently evaluate queries that depend on a sample of data (these are queries that use the SAMPLE clause on the table).
\newline
\par
Apart from primitive column types (integers, floating point numbers, generic strings, dates and booleans), Hive also supports arrays and maps. Additionally, users can compose their own types programmatically from any of the primitives, collections or other user-defined types. The typing system is closely tied to the SerDe and object inspector interfaces. Users can create their own types by implementing their own object inspectors, and using these object inspectors they can create their own SerDes to serialize and deserialize their data into HDFS files).\\
These two interfaces provide the necessary hooks to extend the capabilities of Hive when it comes to understanding other data formats and richer types. Built-in object inspectors like \verb|ListObjectInspector|, \verb|StructObjectInspector| and \verb|MapObjectInspector| provide the necessary primitives to compose richer types in an extensible manner. For maps (associative arrays) and arrays useful built-in functions like size and index operators are provided. The dotted notation is used to navigate nested types.

\subsection{Hive \& SQL Server 2016: Comparison}

When it comes to enterprise-level solutions, every data warehousing product needs to be compared with the de-facto standard of this sector. Considering a pure Windows environments, Microsoft's SQL Server is the most widely used RDBMS and data warehouse.\newline

A comparison has been set up in order to compare the capabilities of Hive with respect to SQL Server, using Hortonworks' TPC-DS\footnote{TPC Benchmark DS is a decision support benchmark that models several generally applicable aspects of a decision support system, including queries and data maintenance. The benchmark provides a representative evaluation of performance as a general purpose decision support system. TPC-DS Version 2 enables emerging technologies, such as Big Data systems, to execute the benchmark.} Testbench implementation with a dataset of 30 GB, results averaged on 5 executions with machines with the following specifications and the following optimizations:
\newline

\begin{table}[!htb]
    \caption{Machine Specifications and Optimizations}
\begin{center}
    \begin{tabular}{|l|p{6cm}|p{5cm}|}
        \hline
        & Hive & SQL Server \\ \hline
        Specifications & 4 LLAP executors (4GB and 2 vCPU each) & 4 cores\\
        & Tez (512MB, 1vCPU) & 16GB RAM\\
        & Slider (512MB, 1vCPU) & Windows Server 2012 R2 \\
        & Total: 17GB, 5 core (10 vCPU) & \\ \hline    
        Optimizations & ORC Format storage, Partitioned tables & Clustered, suggested and ad hoc indices\\ \hline      
    \end{tabular}
\end{center}
\end{table}


Note: Tez and Slider containers, as mentioned in the previous chapter don't actually process any data and are used only for query orchestration.

\subsubsection{Results}

For a total of 65 queries executed, Hive is faster (1.5 to 1200 times) for 38 of them, while SQL Server is better in 19 query executions (1.5 to 54 times), while the remaining 8 queries display similar performances, standing inside the neighbourhood of 1.5x ratio.

Looking at the best results for both sides, we can see that Hive performs better with queries with many WHERE clauses and nested queries, while SQL Server performs better with queries with many GROUP BY, SORT BY and JOIN clauses.

\begin{table}[!htb]
    \begin{center}
            \caption{Hive top 3 best results}
        \begin{tabular}{|l|c|c|c|} \hline
            & Hive & SQL Server & Speedup factor\\ \hline
            query60 & 945 ms & 1.127.549 ms & 1193 \\ \hline
            query7 & 814 ms & 243.714 ms & 299 \\ \hline
            query26 & 1.083 ms & 80.997 ms & 74 \\ \hline
        \end{tabular}
    \bigskip
    \caption{SQL Server top 3 best results}
        \begin{tabular}{|l|c|c|c|} \hline
        & Hive & SQL Server & Speedup factor\\ \hline
        query95 & 37.762 ms & 1.104 ms & 34 \\ \hline
        query22 & 83.305 ms & 7.007 ms & 12 \\ \hline
        query72 & 134.611 ms & 40.043 ms & 3,3 \\ \hline
    \end{tabular}
    \end{center}
\end{table}

\subsubsection{Conclusions}

We can conclude that, generally, Hive applies to different use cases with respect to SQL Server. Whereas, Hive is suitable in scenarios where data sets are very big, where a distributed architecture can be used to scale and speed up the processing of huge swaths of data, SQL Server handles itself progressively worse as data set size increase, becoming an unideal fit in this case. Still, if the data set is limited in size, a single machine architecture is certainly more cost-effective and gives out good performances, having a better and more mature and optimized ACID transactions handling if required.

\section{NoSQL: HBase \& Cassandra}

